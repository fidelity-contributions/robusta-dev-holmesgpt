name: 'Setup KIND Cluster'
description: 'Create and configure a KIND Kubernetes cluster for testing'

inputs:
  cluster-name:
    description: 'Name of the KIND cluster'
    required: false
    default: 'kind'
  wait-for-ready:
    description: 'Whether to wait for cluster to be ready'
    required: false
    default: 'true'

runs:
  using: 'composite'
  steps:
    - name: Install KIND
      shell: bash
      run: |
        curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.31.0/kind-linux-amd64
        chmod +x ./kind
        sudo mv ./kind /usr/local/bin/kind
        kind version

    - name: Create KIND cluster
      shell: bash
      run: |
        cat <<EOF > kind-config.yaml
        kind: Cluster
        apiVersion: kind.x-k8s.io/v1alpha4
        networking:
          disableDefaultCNI: true  # Disable default kindnet to install Calico
        nodes:
        - role: control-plane
          extraPortMappings:
          - containerPort: 30000
            hostPort: 30000
            protocol: TCP
          - containerPort: 30001
            hostPort: 30001
            protocol: TCP
          kubeadmConfigPatches:
          - |
            kind: InitConfiguration
            nodeRegistration:
              kubeletExtraArgs:
                max-pods: "300"
          - |
            kind: KubeProxyConfiguration
            metricsBindAddress: "0.0.0.0:10249"
          - |
            kind: KubeletConfiguration
            maxPods: 300
        EOF

        # Create cluster without waiting for full readiness (CNI needed first)
        kind create cluster --name ${{ inputs.cluster-name }} --config kind-config.yaml

        # Configure kubectl
        kubectl cluster-info --context kind-${{ inputs.cluster-name }}

    - name: Install Calico CNI
      shell: bash
      run: |
        set -e  # Exit on any error
        
        # Calico version configuration
        CALICO_VERSION="v3.31.3"
        
        echo "Installing Calico CNI ${CALICO_VERSION} for NetworkPolicy support..."
        
        # Install Calico operator
        echo "Installing Calico operator..."
        if ! kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/${CALICO_VERSION}/manifests/tigera-operator.yaml; then
          echo "❌ Failed to install Calico operator"
          exit 1
        fi
        
        # Wait for operator to be ready
        echo "Waiting for Calico operator to be ready..."
        
        # First, wait for operator pods to exist (max 60 attempts, 5s each = 300s total)
        OPERATOR_PODS_FOUND=false
        for i in $(seq 1 60); do
          if kubectl get pods -n tigera-operator --no-headers 2>/dev/null | grep -q "tigera-operator"; then
            echo "✅ Calico operator pods found!"
            OPERATOR_PODS_FOUND=true
            break
          else
            echo "⏳ Attempt $i/60: waiting for operator pods to be created, checking in 5s..."
            sleep 5
          fi
        done
        
        if [ "$OPERATOR_PODS_FOUND" = false ]; then
          echo "❌ Calico operator pods failed to appear after 300 seconds"
          kubectl get pods -n tigera-operator
          kubectl describe pods -n tigera-operator
          exit 1
        fi
        
        # Now wait for operator pods to be ready
        echo "Waiting for operator pods to become ready..."
        if ! kubectl wait --for=condition=Ready --timeout=300s -n tigera-operator pods --all; then
          echo "❌ Calico operator failed to become ready"
          kubectl get pods -n tigera-operator
          kubectl describe pods -n tigera-operator
          exit 1
        fi
        
        # Wait for operator to register CRDs
        echo "Waiting for Calico operator CRDs to be registered..."
        CRD_FOUND=false
        for i in $(seq 1 60); do
          if kubectl get crd installations.operator.tigera.io 2>/dev/null; then
            echo "✅ Calico Installation CRD found!"
            CRD_FOUND=true
            break
          else
            echo "⏳ Attempt $i/60: waiting for Installation CRD to be registered, checking in 5s..."
            sleep 5
          fi
        done
        
        if [ "$CRD_FOUND" = false ]; then
          echo "❌ Calico Installation CRD failed to be registered after 300 seconds"
          kubectl get crds | grep -i tigera || echo "No tigera CRDs found"
          kubectl get pods -n tigera-operator
          kubectl describe pods -n tigera-operator
          exit 1
        fi
        
        # Wait for the CRD to be fully established
        echo "Waiting for Installation CRD to be established..."
        if ! kubectl wait --for=condition=Established --timeout=120s crd/installations.operator.tigera.io; then
          echo "❌ Installation CRD failed to become established"
          kubectl get crd installations.operator.tigera.io -o yaml
          exit 1
        fi
        
        # Create custom resource for Calico installation optimized for KIND
        echo "Creating Calico installation resource..."
        if ! cat <<EOF | kubectl create -f -
        apiVersion: operator.tigera.io/v1
        kind: Installation
        metadata:
          name: default
        spec:
          calicoNetwork:
            ipPools:
            - blockSize: 26
              cidr: 10.244.0.0/16  # Match KIND's default pod CIDR
              encapsulation: IPIP
              natOutgoing: Enabled
              nodeSelector: all()
        EOF
        then
          echo "❌ Failed to create Calico installation resource"
          exit 1
        fi
        
        # Wait for Calico to be installed
        echo "Waiting for Calico installation to complete..."
        
        # First, wait for Calico system pods to exist (max 60 attempts, 5s each = 300s total)
        CALICO_PODS_FOUND=false
        for i in $(seq 1 60); do
          if kubectl get pods -n calico-system --no-headers 2>/dev/null | grep -q "calico"; then
            echo "✅ Calico system pods found!"
            CALICO_PODS_FOUND=true
            break
          else
            echo "⏳ Attempt $i/60: waiting for Calico system pods to be created, checking in 5s..."
            sleep 5
          fi
        done
        
        if [ "$CALICO_PODS_FOUND" = false ]; then
          echo "❌ Calico system pods failed to appear after 300 seconds"
          echo "=== Calico system pod status ==="
          kubectl get pods -n calico-system
          echo "=== Calico installation status ==="
          kubectl get installations.operator.tigera.io default -o yaml || echo "Installation resource not found"
          exit 1
        fi
        
        # Now wait for Calico system pods to be ready
        echo "Waiting for Calico system pods to become ready..."
        if ! kubectl wait --for=condition=Ready --timeout=300s -n calico-system pods --all; then
          echo "❌ Calico pods failed to become ready within 300 seconds"
          echo "=== Calico system pod status ==="
          kubectl get pods -n calico-system
          echo "=== Calico installation status ==="
          kubectl get installations.operator.tigera.io default -o yaml || echo "Installation resource not found"
          echo "=== Calico system pod descriptions ==="
          kubectl describe pods -n calico-system
          exit 1
        fi
        
        
        echo "✅ Calico CNI installed successfully with NetworkPolicy support"

    - name: Wait for cluster to be ready
      if: inputs.wait-for-ready == 'true'
      shell: bash
      run: |
        # Wait for all nodes to be ready
        echo "Waiting for nodes to be ready..."
        kubectl wait --for=condition=Ready nodes --all --timeout=300s
        kubectl get nodes

        # Wait for all system pods to be ready
        echo "Waiting for system pods to be ready..."
        kubectl wait --for=condition=Ready pods --all -n kube-system --timeout=300s
        
        

        # Verify cluster is working by creating a test pod
        echo "Creating test pod to verify cluster..."
        kubectl run test-pod --image=busybox:1.35 --restart=Never -- sleep 30
        kubectl wait --for=condition=Ready pod/test-pod --timeout=180s || echo "Warning: Test pod did not become ready in time"
        sleep 2  # Allow time for logs to be available
        kubectl logs test-pod || echo "No logs available yet"
        kubectl delete pod test-pod

        # Show cluster info for debugging
        echo "=== Cluster nodes ==="
        kubectl get nodes -o wide
        echo "=== System pods ==="
        kubectl get pods -n kube-system
        echo "=== Calico pods ==="
        kubectl get pods -n calico-system
        echo "=== CNI configuration ==="
        kubectl get installations.operator.tigera.io default -o yaml | grep -A 10 "status:" || echo "Installation status not available yet"
        echo "=== Cluster is ready for tests with NetworkPolicy support ==="

    - name: Install Helm
      shell: bash
      run: |
        curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
        helm version

    - name: Install kube-prometheus-stack
      shell: bash
      run: |
        set -euo pipefail
        PROM_START_TIME=$(date +%s)
        echo "::group::Installing kube-prometheus-stack"
        echo "Installing kube-prometheus-stack for Prometheus evals..."

        # Add helm repo
        helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
        helm repo update

        # Install with minimal config - pin version for reproducibility
        # Release name 'robusta' creates service 'robusta-kube-prometheus-st-prometheus'
        # Note: additionalScrapeConfigs for annotation-based pod discovery is complex to
        # configure inline. Tests needing custom app metrics should use ServiceMonitors.
        helm install robusta prometheus-community/kube-prometheus-stack \
          --version 72.6.2 \
          --namespace default \
          --set prometheus.prometheusSpec.podMonitorSelectorNilUsesHelmValues=false \
          --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \
          --set prometheus.prometheusSpec.scrapeInterval=15s \
          --set prometheus.prometheusSpec.resources.requests.memory=256Mi \
          --set prometheus.prometheusSpec.resources.requests.cpu=100m \
          --set prometheus.prometheusSpec.resources.limits.memory=512Mi \
          --set prometheus.prometheusSpec.resources.limits.cpu=500m \
          --set alertmanager.enabled=false \
          --set grafana.enabled=false \
          --set kubeStateMetrics.enabled=true \
          --set nodeExporter.enabled=true \
          --wait \
          --timeout 5m

        echo "Waiting for Prometheus to be ready..."
        kubectl wait --for=condition=Ready pods -l app.kubernetes.io/name=prometheus -n default --timeout=300s

        # Verify Prometheus is responding
        echo "Verifying Prometheus API..."
        kubectl run prom-test --image=curlimages/curl:8.8.0 --restart=Never --rm -i --timeout=60s -- \
          curl -s "http://robusta-kube-prometheus-st-prometheus.default.svc.cluster.local:9090/api/v1/status/config" | head -c 200 || true

        echo ""
        echo "=== Prometheus stack installed ==="
        kubectl get pods -l "release=robusta" -n default
        PROM_END_TIME=$(date +%s)
        PROM_ELAPSED=$((PROM_END_TIME - PROM_START_TIME))
        echo "::endgroup::"
        echo "⏱️ kube-prometheus-stack installation took ${PROM_ELAPSED} seconds"

    - name: Install metrics-server
      shell: bash
      run: |
        set -euo pipefail
        METRICS_START_TIME=$(date +%s)
        echo "::group::Installing metrics-server"
        echo "Installing metrics-server for HPA support..."

        # Install metrics-server - pinned version for reproducibility
        kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.7.2/components.yaml

        # Patch for KIND (needs insecure TLS since kubelet certs aren't valid)
        kubectl patch deployment metrics-server -n kube-system \
          --type='json' \
          -p='[{"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kubelet-insecure-tls"}]'

        # Wait for metrics-server to be ready
        echo "Waiting for metrics-server to be available..."
        kubectl wait --for=condition=Available deployment/metrics-server -n kube-system --timeout=120s

        METRICS_END_TIME=$(date +%s)
        METRICS_ELAPSED=$((METRICS_END_TIME - METRICS_START_TIME))
        echo "::endgroup::"
        echo "⏱️ metrics-server installation took ${METRICS_ELAPSED} seconds"
